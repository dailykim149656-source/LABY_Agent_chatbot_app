# Azure Speech STT êµ¬í˜„ ê°€ì´ë“œ

ì‘ì„±ì¼: 2026-01-31
ëŒ€ìƒ: LABY Agent Chatbot (chemical-sample-dashboard)
ë²”ìœ„: Speech-to-Text ê¸°ëŠ¥ êµ¬í˜„

---

## 1) ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STT íë¦„                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ¤ ë§ˆì´í¬  â†’  Azure Speech SDK  â†’  í…ìŠ¤íŠ¸  â†’  Chat API  â”‚
â”‚                                                          â”‚
â”‚  [ìŒì„± ì…ë ¥]    [STT ë³€í™˜]         [ê²°ê³¼]     [ì§ˆë¬¸ ì „ì†¡] â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2) ì‚¬ì „ ì¤€ë¹„

### Azure Speech ë¦¬ì†ŒìŠ¤ ìƒì„±
1. Azure Portal â†’ "Speech" ë¦¬ì†ŒìŠ¤ ìƒì„±
2. ë¦¬ì „: `koreacentral` ê¶Œì¥
3. ê°€ê²© ê³„ì¸µ: Free(F0) ë˜ëŠ” Standard(S0)

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •
```env
# .env.local
NEXT_PUBLIC_AZURE_SPEECH_KEY=your-speech-key
NEXT_PUBLIC_AZURE_SPEECH_REGION=koreacentral
```

### SDK ì„¤ì¹˜
```bash
cd chemical-sample-dashboard
npm install microsoft-cognitiveservices-speech-sdk
```

---

## 3) íŒŒì¼ êµ¬ì¡°

```
chemical-sample-dashboard/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ speech/
â”‚       â”œâ”€â”€ config.ts             # Azure Speech ì„¤ì •
â”‚       â””â”€â”€ speech-to-text.ts     # STT í´ë˜ìŠ¤
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ use-speech-recognition.ts # STT React í›…
```

---

## 4) êµ¬í˜„ ì½”ë“œ

### A. Speech ì„¤ì • (`lib/speech/config.ts`)

```typescript
import * as sdk from "microsoft-cognitiveservices-speech-sdk"

export const SPEECH_KEY = process.env.NEXT_PUBLIC_AZURE_SPEECH_KEY || ""
export const SPEECH_REGION = process.env.NEXT_PUBLIC_AZURE_SPEECH_REGION || "koreacentral"

export function createSpeechConfig() {
  if (!SPEECH_KEY || !SPEECH_REGION) {
    throw new Error("Azure Speech credentials not configured")
  }
  const config = sdk.SpeechConfig.fromSubscription(SPEECH_KEY, SPEECH_REGION)
  config.speechRecognitionLanguage = "ko-KR"
  return config
}
```

### B. STT í´ë˜ìŠ¤ (`lib/speech/speech-to-text.ts`)

```typescript
import * as sdk from "microsoft-cognitiveservices-speech-sdk"
import { createSpeechConfig } from "./config"

export type STTEventHandlers = {
  onRecognizing?: (text: string) => void   // ì‹¤ì‹œê°„ ì¤‘ê°„ ê²°ê³¼
  onRecognized?: (text: string) => void    // ìµœì¢… ê²°ê³¼
  onError?: (error: string) => void
  onStart?: () => void
  onEnd?: () => void
}

export class SpeechToText {
  private recognizer: sdk.SpeechRecognizer | null = null
  private isListening = false

  /**
   * ì—°ì† ìŒì„± ì¸ì‹ ì‹œì‘
   * - ë§ˆì´í¬ì—ì„œ ê³„ì† ë“£ê³  ê²°ê³¼ë¥¼ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
   */
  async startContinuous(handlers: STTEventHandlers): Promise<void> {
    if (this.isListening) return

    const speechConfig = createSpeechConfig()
    const audioConfig = sdk.AudioConfig.fromDefaultMicrophoneInput()
    this.recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig)

    // ì‹¤ì‹œê°„ ì¤‘ê°„ ê²°ê³¼ (ë§í•˜ëŠ” ì¤‘)
    this.recognizer.recognizing = (_, event) => {
      if (event.result.reason === sdk.ResultReason.RecognizingSpeech) {
        handlers.onRecognizing?.(event.result.text)
      }
    }

    // ìµœì¢… ì¸ì‹ ê²°ê³¼ (ë¬¸ì¥ ì™„ë£Œ)
    this.recognizer.recognized = (_, event) => {
      if (event.result.reason === sdk.ResultReason.RecognizedSpeech) {
        handlers.onRecognized?.(event.result.text)
      } else if (event.result.reason === sdk.ResultReason.NoMatch) {
        handlers.onError?.("ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
      }
    }

    // ì—ëŸ¬ ì²˜ë¦¬
    this.recognizer.canceled = (_, event) => {
      if (event.reason === sdk.CancellationReason.Error) {
        handlers.onError?.(event.errorDetails)
      }
      this.stop()
    }

    this.recognizer.sessionStarted = () => {
      this.isListening = true
      handlers.onStart?.()
    }

    this.recognizer.sessionStopped = () => {
      this.isListening = false
      handlers.onEnd?.()
    }

    // ì—°ì† ì¸ì‹ ì‹œì‘
    this.recognizer.startContinuousRecognitionAsync(
      () => console.log("STT started"),
      (err) => handlers.onError?.(err)
    )
  }

  /**
   * ë‹¨ì¼ ë¬¸ì¥ ì¸ì‹ (ì§ˆë¬¸ìš©)
   * - í•œ ë¬¸ì¥ì„ ì¸ì‹í•˜ê³  ìë™ ì¢…ë£Œ
   */
  async recognizeOnce(): Promise<string> {
    return new Promise((resolve, reject) => {
      const speechConfig = createSpeechConfig()
      const audioConfig = sdk.AudioConfig.fromDefaultMicrophoneInput()
      const recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig)

      recognizer.recognizeOnceAsync(
        (result) => {
          recognizer.close()
          if (result.reason === sdk.ResultReason.RecognizedSpeech) {
            resolve(result.text)
          } else if (result.reason === sdk.ResultReason.NoMatch) {
            reject(new Error("ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"))
          } else {
            reject(new Error(`ì¸ì‹ ì‹¤íŒ¨: ${result.reason}`))
          }
        },
        (error) => {
          recognizer.close()
          reject(error)
        }
      )
    })
  }

  /**
   * ìŒì„± ì¸ì‹ ì¤‘ì§€
   */
  stop(): void {
    if (this.recognizer) {
      this.recognizer.stopContinuousRecognitionAsync(
        () => console.log("STT stopped"),
        (err) => console.error("STT stop error:", err)
      )
      this.recognizer.close()
      this.recognizer = null
      this.isListening = false
    }
  }

  /**
   * í˜„ì¬ ì¸ì‹ ì¤‘ì¸ì§€ í™•ì¸
   */
  get listening(): boolean {
    return this.isListening
  }
}
```

### C. React í›… (`hooks/use-speech-recognition.ts`)

```typescript
"use client"

import { useState, useCallback, useRef, useEffect } from "react"
import { SpeechToText } from "@/lib/speech/speech-to-text"

export function useSpeechRecognition() {
  const [isListening, setIsListening] = useState(false)
  const [transcript, setTranscript] = useState("")
  const [interimTranscript, setInterimTranscript] = useState("")
  const [error, setError] = useState<string | null>(null)
  const sttRef = useRef<SpeechToText | null>(null)

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ STT ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
  useEffect(() => {
    sttRef.current = new SpeechToText()
    return () => {
      sttRef.current?.stop()
    }
  }, [])

  /**
   * ì—°ì† ìŒì„± ì¸ì‹ ì‹œì‘
   */
  const startListening = useCallback(async () => {
    setError(null)
    setTranscript("")
    setInterimTranscript("")

    try {
      await sttRef.current?.startContinuous({
        onRecognizing: (text) => setInterimTranscript(text),
        onRecognized: (text) => {
          setTranscript((prev) => (prev ? prev + " " + text : text))
          setInterimTranscript("")
        },
        onStart: () => setIsListening(true),
        onEnd: () => setIsListening(false),
        onError: (err) => setError(err),
      })
    } catch (err) {
      setError(err instanceof Error ? err.message : "STT ì‹œì‘ ì‹¤íŒ¨")
    }
  }, [])

  /**
   * ìŒì„± ì¸ì‹ ì¤‘ì§€
   */
  const stopListening = useCallback(() => {
    sttRef.current?.stop()
    setIsListening(false)
  }, [])

  /**
   * ë‹¨ì¼ ë¬¸ì¥ ì¸ì‹ (ì§ˆë¬¸ìš©)
   */
  const recognizeOnce = useCallback(async (): Promise<string> => {
    setError(null)
    setIsListening(true)
    try {
      const result = await sttRef.current?.recognizeOnce()
      return result || ""
    } catch (err) {
      const message = err instanceof Error ? err.message : "ì¸ì‹ ì‹¤íŒ¨"
      setError(message)
      throw err
    } finally {
      setIsListening(false)
    }
  }, [])

  /**
   * ìƒíƒœ ì´ˆê¸°í™”
   */
  const reset = useCallback(() => {
    setTranscript("")
    setInterimTranscript("")
    setError(null)
  }, [])

  return {
    isListening,
    transcript,
    interimTranscript,
    error,
    startListening,
    stopListening,
    recognizeOnce,
    reset,
  }
}
```

---

## 5) ì‚¬ìš© ì˜ˆì‹œ

### ë‹¨ì¼ ë¬¸ì¥ ì¸ì‹ (ìŒì„± ì§ˆë¬¸)

```typescript
"use client"

import { useSpeechRecognition } from "@/hooks/use-speech-recognition"
import { Mic, MicOff } from "lucide-react"
import { Button } from "@/components/ui/button"

export function VoiceInput({ onResult }: { onResult: (text: string) => void }) {
  const { isListening, error, recognizeOnce } = useSpeechRecognition()

  const handleClick = async () => {
    try {
      const text = await recognizeOnce()
      if (text) {
        onResult(text)
      }
    } catch {
      // ì—ëŸ¬ëŠ” í›…ì—ì„œ ì²˜ë¦¬ë¨
    }
  }

  return (
    <div>
      <Button
        variant="ghost"
        size="icon"
        onClick={handleClick}
        disabled={isListening}
      >
        {isListening ? (
          <Mic className="size-5 animate-pulse text-red-500" />
        ) : (
          <MicOff className="size-5" />
        )}
      </Button>
      {error && <p className="text-sm text-red-500">{error}</p>}
    </div>
  )
}
```

### ì—°ì† ì¸ì‹ (ì‹¤ì‹œê°„ ìë§‰)

```typescript
"use client"

import { useSpeechRecognition } from "@/hooks/use-speech-recognition"

export function LiveTranscript() {
  const {
    isListening,
    transcript,
    interimTranscript,
    startListening,
    stopListening,
  } = useSpeechRecognition()

  return (
    <div>
      <button onClick={isListening ? stopListening : startListening}>
        {isListening ? "ì¤‘ì§€" : "ì‹œì‘"}
      </button>
      <div>
        <p>{transcript}</p>
        <p className="text-gray-400">{interimTranscript}</p>
      </div>
    </div>
  )
}
```

### Chat ì¸í„°í˜ì´ìŠ¤ ì—°ë™

```typescript
"use client"

import { useSpeechRecognition } from "@/hooks/use-speech-recognition"
import { postChatMessage } from "@/lib/data/chat"

export function VoiceChatInput({ roomId }: { roomId: string }) {
  const { isListening, recognizeOnce } = useSpeechRecognition()

  const handleVoiceInput = async () => {
    try {
      // 1. ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
      const question = await recognizeOnce()

      if (question) {
        // 2. ì±—ë´‡ APIë¡œ ì§ˆë¬¸ ì „ì†¡
        await postChatMessage(roomId, {
          message: question,
          sender_type: "guest",
        })
      }
    } catch (error) {
      console.error("Voice input failed:", error)
    }
  }

  return (
    <button onClick={handleVoiceInput} disabled={isListening}>
      {isListening ? "ë“£ëŠ” ì¤‘..." : "ğŸ¤ ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸"}
    </button>
  )
}
```

---

## 6) ì§€ì› ì–¸ì–´

| ì½”ë“œ | ì–¸ì–´ |
|------|------|
| `ko-KR` | í•œêµ­ì–´ (ê¸°ë³¸) |
| `en-US` | ì˜ì–´ (ë¯¸êµ­) |
| `ja-JP` | ì¼ë³¸ì–´ |
| `zh-CN` | ì¤‘êµ­ì–´ (ê°„ì²´) |

ì–¸ì–´ ë³€ê²½:
```typescript
config.speechRecognitionLanguage = "en-US"
```

---

## 7) ë¸Œë¼ìš°ì € í˜¸í™˜ì„±

| ë¸Œë¼ìš°ì € | ì§€ì› |
|---------|------|
| Chrome | âœ… |
| Edge | âœ… |
| Firefox | âœ… |
| Safari | âš ï¸ ì œí•œì  |

**ì£¼ì˜ì‚¬í•­:**
- HTTPS í•„ìˆ˜ (ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œ)
- ì‚¬ìš©ì ì œìŠ¤ì²˜ í›„ ë§ˆì´í¬ ì ‘ê·¼ ê°€ëŠ¥ (ìë™ ì‹œì‘ ë¶ˆê°€)

---

## 8) ì˜ˆìƒ ë¹„ìš©

| í•­ëª© | ë¬´ë£Œ ê³„ì¸µ (F0) | ìœ ë£Œ ê³„ì¸µ (S0) |
|------|---------------|---------------|
| STT | 5ì‹œê°„/ì›” | $1/ì‹œê°„ |
| ì‹¤ì‹œê°„ STT | 5ì‹œê°„/ì›” | $1.40/ì‹œê°„ |

---

## 9) ë¬¸ì œ í•´ê²°

### ë§ˆì´í¬ ê¶Œí•œ ì˜¤ë¥˜
```
Error: Microphone access denied
```
â†’ ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©

### ì¸ì‹ ì•ˆë¨
```
NoMatch: Speech could not be recognized
```
â†’ ë§ˆì´í¬ ì…ë ¥ ë³¼ë¥¨ í™•ì¸, ì¡°ìš©í•œ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸

### SDK ë¡œë“œ ì‹¤íŒ¨
```
Error: Speech SDK not loaded
```
â†’ `npm install microsoft-cognitiveservices-speech-sdk` í™•ì¸

---

## 10) ë‹¤ìŒ ë‹¨ê³„

- [ ] TTS êµ¬í˜„ (ì‘ë‹µ ìŒì„± ì¶œë ¥)
- [ ] Wake Word êµ¬í˜„ ("Hey LABY")
- [ ] ì‚¬ê³  ì•Œë¦¼ TTS ì—°ë™
